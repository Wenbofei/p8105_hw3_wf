---
title: "P8105 HW3"
author: "Wenbo Fei"
date: "10/10/2020"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(p8105.datasets)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

```{r P1}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.  Each observation is an item specific record. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. For example, the first order record shows that in order1 made by user112108 at 10:00 on the forth day of a week after 9 days of his last purchase while this is his 4th order, a product "Bulgarian Yogurt" with ID 49302 is bought; this is an reodered item and it was the first one to be added in his cart; it belongs to dairy eggs department(16) on the yogurt aisle(120).

```{r P1Q1}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```

There are 134 aisles, and fresh vegetables are the most items ordered from.

Let's see the plot about number of items ordered in each aisle.

```{r P1Q2}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs( y = "number of items ordered")
```

Let's see a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r P1Q3}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

Let's see a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r P1Q4}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

# Problem 2
```{r P2_load_data, message=FALSE}
accel_data = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    names_prefix = "activity_",
    values_to = "activity_count" ) %>%
    mutate( weekend = ifelse(day %in% c("Saturday","Sunday"), 1, 0),
            day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>%
  mutate_at(vars(week:day_id), as.factor) %>%
  mutate_at("minute_of_day", as.integer) %>%
  mutate_at("weekend", as.logical) 
  
head(accel_data)
```

Our final dataset includes `r nrow(accel_data)` observations with `r ncol(accel_data)` variables. Each observation has a "week" variable indicating which week, a unique"day_id" variable for each specific day, a "day" variable indicating the day of the week, a "minute_of_day" variable indicating which minute of the day is the record about, a "activity_court" recoding the activity counts for the minute, and a "weekend" indicating whether it's a weekend or a weekday.

Let's see a table showing daily total activity.

```{r P2Q2}
accel_data %>%
  group_by(day_id) %>%
  summarize(total_activity = sum(activity_count)) %>%
  knitr::kable()
```

I can't see any apparent increasing or decreasing trend from this table. Looks like except for 3 days, all the other days have same order of magnitude of total activity count with fluctuation. We may need further analysis and visulization to find systematic trend.

Let's see a plot on activity over the course of the day, while each line represents a day and each color represent a specific day in a week.
```{r P2Q3}
accel_data %>%
  group_by(day_id) %>%
  ggplot(aes(x = minute_of_day, y = activity_count, color = day)) + 
    geom_point(alpha = .5) + geom_line(alpha = .5) + 
    theme(legend.position = "bottom") +
  labs(title = "Figure 1: activity over the course of the day")
```

There activity over the course of the day varies for different time in a day and different day in a week. From around 350 minutes, the person maintain a moderate activity count, compared to 270 minutes in weekdays, which might mean that he gets up earlier in weekdays than weekend. The peak of activity on weekend are between 600-750 minutes, around 1000 minutes and some 1200 minutes, while the major peak for weekdays are around 1250 minutes, some around 500 minutes, this may indicate that the person do exercises in different time of a day.

# Problem 3

```{r P3data}
data("ny_noaa")
```

These data were accessed from the NOAA National Climatic Data Center containing weather information for all  New York state weather stations from 1981 to 2010. It contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables, while "id" represents weather station ID, "date" represents date of observation, "prcp" represents precipitation (tenths of mm), "snow" represents snowfall (mm),"snwd" represents snow depth (mm), "tmax" maximum temperature (tenths of degrees C), "tmin" minimum temperature (tenths of degrees C). Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data. Of all observations, 52.9% contain at least one missing value. In addition, 2.8% have missing values for all five key variables, and for this proportion, we nearly get nothing from this data.


```{r P3_cleaning}
noaa_data <- ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, c("year", "month", "day"), sep = "-") %>% 
  mutate(tmax = as.double(tmax) / 10, 
         tmin = as.double(tmin) / 10, 
         prcp = prcp / 10,
         month = month.name[as.integer(month)], 
         year = as.integer(year))

quantile(pull(noaa_data, snow), na.rm = T)
sum(pull(noaa_data, snow)<0, na.rm = T)
```

From the quantile, we can see that at least over 50% of snowfall data are 0, so 0 is the most commonly observed values. This is reasonable as we know that NY usually have snow in winter season while no snow in other 3 seasons. However, there's a negative snowfall, which might be a mistake.

Let's see a two-panel plot showing the average max temperature in January and in July in each station across years.  Each point represents average tmax over the month in the year in specific station.

```{r P3_plot1, message=F}
noaa_data %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(year, month, id) %>% 
  summarize(avg_temp = mean(tmax, na.rm = TRUE)) %>% 
  drop_na() %>%
  ggplot(aes(x = year, y = avg_temp)) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(se = F) +
  facet_grid(~ month) + 
  labs(title = "Figure 2: Average maximum temperature (°C) in January and July, 1981-2010", 
       x = "Year", 
       y = "Average Max Daily Temperature (°C)") 
```

There's an continuous increase for the average max temperature in January from 1980-1990, and then it tends to flutuate around zero degrees while the temperature for July varies around 27 degrees. In addition, the average maximum temperature for January appears to have more variation compared to that of July. Looks like there is one station has much lower average tmax(10 degress than the min of all the other station, and nearly 15 degrees from the center) in July 1988, we need to further check the data and other information to determine whether it's an outlier.

Let's see a two-panel plot showing 
(i) tmax vs tmin for the full dataset. For this plot, since there are too many data points, we use a heatmap.
(ii) the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r P3Q3, message=F}
library(plotly) #for hexbin()
library(patchwork) #for wrap_elements()
temp_hex <- noaa_data %>%
  ggplot(aes(x = tmin, y = tmax)) +
    geom_hex() + 
    theme(legend.position = "bottom",
          legend.key.height = unit(0.1, "in"),
          legend.key.width = unit(0.4, "in")) +
    labs(title = "Heatmap of daily tmax vs. tmin",
         x = "tmin (°C)", 
         y = "tmax (°C)") 

snow_density <- noaa_data %>%
  filter(snow > 0 & snow < 100) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(x = snow, color = year)) +
  geom_density(alpha = 0.01) +
  labs(title = "Density plot of snowfall(mm)",
         x = "Snowfall (mm)", 
         y = "Density") + 
  theme(legend.key.size = unit(0.05, "in"))

final_plot <- temp_hex + snow_density 

wrap_elements(final_plot) + ggtitle("Figure 3: Heatmap and density plots")
```

From the heatmap, the lighter the color means the higher density, it's obvious that there is positive linear relationship between tmin and tmax.

From the density plot, it can be seen that there are local maximum in density around 20, 25, 50, and 75 mm. One can also notice that the heights of later 25, 50, 75 peaks has decreased over each the years -- the lighter the color, the lower the peak. This might indicate the result of climate change resulting in less snowfall.